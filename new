# https://tea.xyz/what-is-this-file
---
version: 1.0.0
codeOwners:
  - '0x1c1E0698A7102db443AfaF153E7E06fE74725F1C'
quorum: 1
gist to pull all tweets and retweets in your agent 


import requests
import json
from datetime import datetime
import time
from typing import Dict, List, Optional

class TwitterScraper:
    def __init__(self, bearer_token: str):
        self.bearer_token = bearer_token
        self.base_url = "https://api.twitter.com/2"
        self.headers = {
            "Authorization": f"Bearer {bearer_token}",
            "User-Agent": "TwitterScraperBot/1.0"
        }

    def get_user_id(self, username: str) -> Optional[str]:
        """Get user ID from username."""
        endpoint = f"{self.base_url}/users/by/username/{username}"
        response = requests.get(endpoint, headers=self.headers)
        
        if response.status_code != 200:
            print(f"Error getting user ID: {response.status_code}")
            return None
            
        return response.json()['data']['id']

    def get_tweets(self, user_id: str, max_results: int = 100) -> List[Dict]:
        """Get all types of tweets for a user with rate limit handling."""
        all_tweets = []
        pagination_token = None
        max_retries = 5
        base_delay = 60  # Base delay in seconds
        
        # Track counts for each type
        counts = {"post": 0, "retweet": 0, "quote_tweet": 0}
        
        print("Fetching tweets, retweets, and quote tweets...")
        
        while True:
            endpoint = f"{self.base_url}/users/{user_id}/tweets"
            params = {
                "max_results": max_results,
                "tweet.fields": "created_at,referenced_tweets,author_id",
                "expansions": "referenced_tweets.id,referenced_tweets.id.author_id",
                "exclude": "replies",  # Exclude replies to focus on posts, RTs, and QTs
                "pagination_token": pagination_token
            }
            
            retry_count = 0
            success = False
            
            while retry_count < max_retries and not success:
                try:
                    response = requests.get(endpoint, headers=self.headers, params=params)
                    
                    # Check rate limit headers
                    remaining = int(response.headers.get('x-rate-limit-remaining', 0))
                    reset_time = int(response.headers.get('x-rate-limit-reset', 0))
                    
                    if response.status_code == 429:
                        wait_time = reset_time - int(time.time())
                        if wait_time > 0:
                            print(f"Rate limit hit. Waiting {wait_time} seconds...")
                            time.sleep(wait_time + 1)  # Add 1 second buffer
                            continue
                            
                    response.raise_for_status()
                    data = response.json()
                    
                    if "data" not in data:
                        return all_tweets

                    # Count items in this batch
                    batch_counts = {"post": 0, "retweet": 0, "quote_tweet": 0}
                    
                    for tweet in data["data"]:
                        tweet_type = "post"
                        referenced_tweets = tweet.get("referenced_tweets", [])
                        
                        if referenced_tweets:
                            ref_type = referenced_tweets[0]["type"]
                            if ref_type == "retweeted":
                                tweet_type = "retweet"
                            elif ref_type == "quoted":
                                tweet_type = "quote_tweet"
                        
                        batch_counts[tweet_type] += 1
                        
                        processed_tweet = {
                            "type": tweet_type,
                            "date": tweet["created_at"],
                            "content": tweet["text"],
                            "id": tweet["id"]
                        }
                        
                        # Add reference info for RTs and QTs
                        if tweet_type in ["retweet", "quote_tweet"] and "includes" in data:
                            referenced_id = referenced_tweets[0]["id"]
                            for ref_tweet in data["includes"].get("tweets", []):
                                if ref_tweet["id"] == referenced_id:
                                    processed_tweet["referenced_content"] = ref_tweet["text"]
                                    processed_tweet["referenced_date"] = ref_tweet["created_at"]
                                    break
                        
                        all_tweets.append(processed_tweet)
                        counts[tweet_type] += 1

                    # Print batch info
                    print(f"\nBatch received: {len(data['data'])} tweets "
                          f"({batch_counts['post']} posts, "
                          f"{batch_counts['retweet']} RTs, "
                          f"{batch_counts['quote_tweet']} QTs)")
                    print(f"Running total: {len(all_tweets)} tweets "
                          f"({counts['post']} posts, "
                          f"{counts['retweet']} RTs, "
                          f"{counts['quote_tweet']} QTs)")

                    if "next_token" not in data["meta"]:
                        return all_tweets
                        
                    pagination_token = data["meta"]["next_token"]
                    
                    # Adaptive rate limiting
                    if remaining < 10:
                        wait_time = (reset_time - int(time.time())) / remaining
                        print(f"Low on rate limits. Waiting {wait_time:.1f} seconds...")
                        time.sleep(wait_time)
                    else:
                        time.sleep(2)  # Standard delay between successful requests
                        
                    success = True
                    
                except requests.exceptions.RequestException as e:
                    retry_count += 1
                    if retry_count == max_retries:
                        print(f"Max retries reached. Error: {e}")
                        return all_tweets
                        
                    wait_time = base_delay * (2 ** (retry_count - 1))  # Exponential backoff
                    print(f"Request failed. Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)

        return all_tweets

    def save_to_json(self, tweets: List[Dict], filename: str):
        """Save tweets to JSON file."""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(tweets, f, ensure_ascii=False, indent=2)

def main():
    # Replace with your bearer token
    BEARER_TOKEN = ""
    
    # Initialize scraper
    scraper = TwitterScraper(BEARER_TOKEN)
    
    # Get username from user input
    username = input("Enter Twitter username (without @): ")
    
    # Get user ID
    user_id = scraper.get_user_id(username)
    if not user_id:
        print("User not found")
        return
    
    # Get tweets
    print(f"Fetching tweets for @{username}...")
    print("Note: Twitter API limits access to approximately the last 3200 tweets")
    tweets = scraper.get_tweets(user_id)
    
    if len(tweets) >= 3200:
        print("\nReached Twitter's historical limit (3200 tweets)")
    elif len(tweets) > 0:
        print(f"\nReached the end of available tweets ({len(tweets)} total)")
    
    # Save to JSON
    filename = f"{username}_tweets_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    scraper.save_to_json(tweets, filename)
    print(f"Saved {len(tweets)} tweets to {filename}")

if __name__ == "__main__":
    main()